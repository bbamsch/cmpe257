{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=open('cornell/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_ids = {}\n",
    "for line in lines:\n",
    "    line_lst = line.split(' +++$+++ ')\n",
    "    if len(line_lst) == 5:\n",
    "        line_ids[line_lst[0]] = line_lst[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = open('cornell/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []\n",
    "for line in conv:\n",
    "    line_lst = line.split(' +++$+++ ')\n",
    "    line = line_lst[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    convs.append(line.split(','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate question and answers \n",
    "questions = []\n",
    "answers = []\n",
    "for conv in convs:\n",
    "    if len(conv) %2 != 0:\n",
    "        conv = conv[:-1]    \n",
    "    for i in range(len(conv)):\n",
    "        if i%2 == 0:\n",
    "            questions.append(line_ids[conv[i]])\n",
    "        else:\n",
    "            answers.append(line_ids[conv[i]]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to lowercase\n",
    "questions = [ line.lower() for line in questions ]\n",
    "answers = [ line.lower() for line in answers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can we make this quick?  roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad.  again.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"well, i thought we'd start with pronunciation, if that's okay with you.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters \n",
    "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' # space is included in whitelist\n",
    "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
    "def filter_line(line, whitelist):\n",
    "    return ''.join([ ch for ch in line if ch in whitelist ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [ filter_line(line, EN_WHITELIST) for line in questions ]\n",
    "answers = [ filter_line(line, EN_WHITELIST) for line in answers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well i thought wed start with pronunciation if thats okay with you'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(qseq, aseq):\n",
    "    filtered_q, filtered_a = [], []\n",
    "    raw_data_len = len(qseq)\n",
    "    for i in range(raw_data_len):\n",
    "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
    "        if qlen >= 2 and qlen <= 25:\n",
    "            if alen >= 2 and alen <= 25:\n",
    "                filtered_q.append(qseq[i])\n",
    "                filtered_a.append(aseq[i])\n",
    "\n",
    "    return filtered_q, filtered_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = filter_data(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "question_tokens = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in questions ]\n",
    "answer_tokens   = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in answers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can',\n",
       " 'we',\n",
       " 'make',\n",
       " 'this',\n",
       " 'quick',\n",
       " 'roxanne',\n",
       " 'korrine',\n",
       " 'and',\n",
       " 'andrew',\n",
       " 'barrett',\n",
       " 'are',\n",
       " 'having',\n",
       " 'an',\n",
       " 'incredibly',\n",
       " 'horrendous',\n",
       " 'public',\n",
       " 'break',\n",
       " 'up',\n",
       " 'on',\n",
       " 'the',\n",
       " 'quad',\n",
       " 'again']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'wed',\n",
       " 'start',\n",
       " 'with',\n",
       " 'pronunciation',\n",
       " 'if',\n",
       " 'thats',\n",
       " 'okay',\n",
       " 'with',\n",
       " 'you']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization: Words to index and index to words\n",
    "UNK = 'unk'\n",
    "freq_dist = nltk.FreqDist(itertools.chain(*(question_tokens + answer_tokens)))\n",
    "vocab = freq_dist.most_common(8000)\n",
    "index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
    "word2index = dict([(w,i) for i,w in enumerate(index2word)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unk(qtokenized, atokenized, w2idx):\n",
    "    data_len = len(qtokenized)\n",
    "\n",
    "    filtered_q, filtered_a = [], []\n",
    "\n",
    "    for qline, aline in zip(qtokenized, atokenized):\n",
    "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
    "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
    "        if unk_count_a <= 2:\n",
    "            if unk_count_q > 0:\n",
    "                if unk_count_q/len(qline) > 0.2:\n",
    "                    pass\n",
    "            filtered_q.append(qline)\n",
    "            filtered_a.append(aline)\n",
    "\n",
    "    return filtered_q, filtered_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter unknowns\n",
    "qtokenized, atokenized = filter_unk(question_tokens, answer_tokens, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can',\n",
       " 'we',\n",
       " 'make',\n",
       " 'this',\n",
       " 'quick',\n",
       " 'roxanne',\n",
       " 'korrine',\n",
       " 'and',\n",
       " 'andrew',\n",
       " 'barrett',\n",
       " 'are',\n",
       " 'having',\n",
       " 'an',\n",
       " 'incredibly',\n",
       " 'horrendous',\n",
       " 'public',\n",
       " 'break',\n",
       " 'up',\n",
       " 'on',\n",
       " 'the',\n",
       " 'quad',\n",
       " 'again']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'wed',\n",
       " 'start',\n",
       " 'with',\n",
       " 'pronunciation',\n",
       " 'if',\n",
       " 'thats',\n",
       " 'okay',\n",
       " 'with',\n",
       " 'you']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(question_tokens)\n",
    "# numpy arrays to store indices\n",
    "idx_q = np.zeros([data_len, 25], dtype=np.int32) \n",
    "idx_a = np.zeros([data_len, 25], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98706, 25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98706"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, lookup, maxlen):\n",
    "    indices = []\n",
    "    for word in seq:\n",
    "        if word in lookup:\n",
    "            indices.append(lookup[word])\n",
    "        else:\n",
    "            indices.append(lookup[UNK])\n",
    "    return indices + [0]*(maxlen - len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data_len):\n",
    "    q_indices = pad_seq(question_tokens[i], word2index, 25)\n",
    "    a_indices = pad_seq(answer_tokens[i], word2index, 25)\n",
    "    idx_q[i] = np.array(q_indices)\n",
    "    idx_a[i] = np.array(a_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  52,   22,  114,   17,  897,    1,    1,   11, 3963, 7516,   28,\n",
       "        410,   81, 3677,    1, 1255,  501,   55,   29,    4,    1,  183,\n",
       "          0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43,   3, 140, 607, 331,  34,   1,  46,  49, 106,  34,   2,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('idx_q.npy', idx_q)\n",
    "np.save('idx_a.npy', idx_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "        'w2idx' : word2index,\n",
    "        'idx2w' : index2word,\n",
    "        'freq_dist' : freq_dist\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(PATH=''):\n",
    "    # read data control dictionaries\n",
    "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    # read numpy arrays\n",
    "    idx_q = np.load(PATH + 'idx_q.npy')\n",
    "    idx_a = np.load(PATH + 'idx_a.npy')\n",
    "    return metadata, idx_q, idx_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n",
    "    # number of examples\n",
    "    data_len = len(x)\n",
    "    lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
    "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
    "    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n",
    "\n",
    "    return (trainX,trainY), (testX,testY), (validX,validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY), (validX, validY) = split_dataset(idx_q, idx_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 52  22 114 ...   0   0   0]\n",
      " [ 24   4   1 ...   0   0   0]\n",
      " [ 36 573  12 ...   0   0   0]\n",
      " ...\n",
      " [ 22 162 218 ...   0   0   0]\n",
      " [ 26 218  13 ...   0   0   0]\n",
      " [426  12   8 ...   0   0   0]]\n",
      "[[ 43   3 140 ...   0   0   0]\n",
      " [106 102  45 ...   0   0   0]\n",
      " [340   7   0 ...   0   0   0]\n",
      " ...\n",
      " [  3 234  17 ... 192   0   0]\n",
      " [  1 234   1 ... 256   1   0]\n",
      " [  1  83  74 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainX)\n",
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen_x = trainX.shape[-1]\n",
    "seqlen_y = trainY.shape[-1]\n",
    "input_vocab = len(idx_q)\n",
    "output_vocab = len(idx_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "98706\n",
      "98706\n"
     ]
    }
   ],
   "source": [
    "print(seqlen_x)\n",
    "print(seqlen_y)\n",
    "print(input_vocab)\n",
    "print(output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbamsch/.virtualenvs/cmpe257/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "enc_ip = [ tf.placeholder(shape=[None,],\n",
    "                          dtype=tf.int64,\n",
    "                          name='ei_{}'.format(t)) for t in range(seqlen_y) ]\n",
    "\n",
    "labels = [ tf.placeholder(shape=[None,],\n",
    "                          dtype=tf.int64,\n",
    "                          name='ei_{}'.format(t)) for t in range(seqlen_y) ]\n",
    "\n",
    "dec_ip = [ tf.zeros_like(enc_ip[0], dtype=tf.int64, name='GO') ] + labels[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "    tf.nn.rnn_cell.BasicLSTMCell(emb_dim, state_is_tuple=True),\n",
    "    output_keep_prob=keep_prob)\n",
    "\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([basic_cell]*num_layers, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_outputs, decode_states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "        enc_ip,\n",
    "        dec_ip,\n",
    "        stacked_lstm,\n",
    "        input_vocab,\n",
    "        output_vocab,\n",
    "        emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "\n",
    "loss_weights = [ tf.ones_like(label, dtype=tf.float32) \n",
    "                 for label in labels ]\n",
    "\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "    decode_outputs, labels, loss_weights, output_vocab)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sess, X):\n",
    "    feed_dict = {self.enc_ip[t]: X[t] for t in range(seqlen_y)}\n",
    "    feed_dict[self.keep_prob] = 1.\n",
    "    dec_op_v = sess.run(self.decode_outputs_test, feed_dict)\n",
    "    # dec_op_v is a list; also need to transpose 0,1 indices \n",
    "    #  (interchange batch_size and timesteps dimensions\n",
    "    dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "    # return the index of item with highest probability\n",
    "    return np.argmax(dec_op_v, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
